{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnTLm04zkzFM"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "executionInfo": {
     "elapsed": 161,
     "status": "ok",
     "timestamp": 1625785213216,
     "user": {
      "displayName": "Yash Thakur",
      "photoUrl": "",
      "userId": "12694896413693995597"
     },
     "user_tz": 240
    },
    "id": "e4KRoFt9rzQR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1625785214599,
     "user": {
      "displayName": "Yash Thakur",
      "photoUrl": "",
      "userId": "12694896413693995597"
     },
     "user_tz": 240
    },
    "id": "dSwxCMZunCO4",
    "outputId": "701eef7e-9e04-41ec-80f3-343cf71664ab"
   },
   "outputs": [],
   "source": [
    "# for pkg in ['stopwords', 'punkt', 'wordnet']:\n",
    "#   nltk.download(pkg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJ9B-DVQksPq"
   },
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1625787190589,
     "user": {
      "displayName": "Yash Thakur",
      "photoUrl": "",
      "userId": "12694896413693995597"
     },
     "user_tz": 240
    },
    "id": "A5JjA4pocyDC"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Split rows of train.csv into [text, label] if possible\n",
    "def split_text_label(text):\n",
    "  # The first group is the text, the second is the label at the end\n",
    "  match = re.match(r'^(.*)\\s+?([^\\s]+)$', text)\n",
    "  \n",
    "  if not match: raise ValueError(f'format wrong arg={text}')\n",
    "  \n",
    "  text = match.group(1)\n",
    "  label = match.group(2)\n",
    "  \n",
    "  if label == '1':\n",
    "    label = 1.0\n",
    "  elif label == '0':\n",
    "    label = 0.0\n",
    "  elif label == 'label':\n",
    "    return None # There's a line \"content label\" in the data\n",
    "  else:\n",
    "    raise ValueError(f'label wrong label={label}, text={text}')\n",
    "  \n",
    "  return [label, text.strip()]\n",
    "\n",
    "def mapl(f, list):\n",
    "  '''Map a list eagerly'''\n",
    "  return [f(elem) for elem in list]\n",
    "\n",
    "def load_csv(file_name):\n",
    "  # Data obtained from https://www.kaggle.com/c/fakenewskdd2020/data\n",
    "  with open(f'fakenewskdd2020/{file_name}.csv', 'r', encoding='utf8') as csv:\n",
    "    next(csv) # Skip the header with the column titles\n",
    "    return list(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "8um1nfIZTEWt"
   },
   "outputs": [],
   "source": [
    "train_orig = mapl(split_text_label, load_csv('train'))\n",
    "train_orig = [res for res in train_raw if res]\n",
    "\n",
    "X_train_orig = mapl(lambda text_label: text_label[1], train_orig)\n",
    "y_train_orig = mapl(lambda text_label: text_label[0], train_orig)\n",
    "X_test_orig = mapl(lambda line: re.sub(re.compile('^\\\\d+\\t'), '', line), load_csv('test'))\n",
    "y_test_orig = mapl(lambda line: float(re.sub(re.compile('^\\\\d+,|\\n'), '', line)), load_csv('sample_submission'))\n",
    "\n",
    "assert len(X_train_orig) == len(y_train_orig)\n",
    "assert len(X_test_orig) == len(y_test_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure there are an equal number of fake and true articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pos_neg(X, y, num_vals):\n",
    "  pos = [0] * num_vals\n",
    "  neg = [0] * num_vals\n",
    "  pos_ind, neg_ind = 0, 0\n",
    "  for i in range(len(X)):\n",
    "    if y[i]:\n",
    "      if pos_ind < num_vals:\n",
    "        pos[pos_ind] = X[i]\n",
    "        pos_ind += 1\n",
    "    else:\n",
    "      if neg_ind < num_vals:\n",
    "        neg[neg_ind] = X[i]\n",
    "        neg_ind += 1\n",
    "  \n",
    "  # Associate each set of texts with their labels\n",
    "  pos = mapl(lambda text: [1.0, text], pos)\n",
    "  neg = mapl(lambda text: [0.0, text], neg)\n",
    "  \n",
    "  # Put them together into a DataFrame\n",
    "  df = pd.DataFrame(pos + neg, columns=['label', 'text'])\n",
    "  # Shuffle it just in case (from https://stackoverflow.com/a/34879805)\n",
    "  df = df.sample(frac=1).reset_index(drop=True)\n",
    "  \n",
    "  X = df['text'].to_list()\n",
    "  y = np.array(mapl(bool, df['label'].to_list()))\n",
    "\n",
    "  return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = min(y_train_orig.count(0.0), y_train_orig.count(1.0))\n",
    "num_test = min(y_test_orig.count(0.0), y_test_orig.count(1.0))\n",
    "\n",
    "X_train_trimmed, y_train = split_pos_neg(X_train_orig, y_train_orig, num_train)\n",
    "X_test_trimmed, y_test = split_pos_neg(X_test_orig, y_test_orig, num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0ZAqfJHk4we"
   },
   "source": [
    "# Processing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Angad's LDA_Demo.ipynb\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "stemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(document):\n",
    "  # Remove all the special characters\n",
    "  document = re.sub(r'\\W', ' ', str(document))\n",
    "  # remove all single characters\n",
    "  document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "  # Remove single characters from the start\n",
    "  document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "  # Substituting multiple spaces with single space\n",
    "  document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "  # Converting to Lowercase\n",
    "  document = document.lower()\n",
    "  # Lemmatization\n",
    "  tokens = mapl(stemmer.lemmatize, document.split())\n",
    "  tokens = [word for word in tokens if len(word) > 3  and word not in en_stop]\n",
    "\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean = mapl(preprocess_text, X_train_trimmed)\n",
    "X_test_clean = mapl(preprocess_text, X_test_trimmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 100000\n",
      "Vocabulary size: 49272\n"
     ]
    }
   ],
   "source": [
    "# Much of the following is from https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_clean)\n",
    "\n",
    "max_length = max([len(text) for text in X_train_clean])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length:', length)\n",
    "print('Vocabulary size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (4028, 8718)\n",
      "X_test shape:  (1234, 8718)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 3463,  1440,   669, ...,     0,     0,     0],\n",
       "        [   12,   674,  1574, ...,     0,     0,     0],\n",
       "        [  340,  4364,     0, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [ 5145,   104, 49263, ...,     0,     0,     0],\n",
       "        [  498,   524,   607, ...,     0,     0,     0],\n",
       "        [12832,  5372,  3753, ...,     0,     0,     0]]),\n",
       " array([[ 1186,  1186,   600, ...,     0,     0,     0],\n",
       "        [   81,   434,    93, ...,     0,     0,     0],\n",
       "        [ 4744,  2133,  6652, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [   23,    16,    23, ...,     0,     0,     0],\n",
       "        [  860, 29915,   393, ...,     0,     0,     0],\n",
       "        [  258,   406, 14045, ...,     0,     0,     0]]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(tokenizer, lines, length):\n",
    "  encoded = tokenizer.texts_to_sequences(lines)\n",
    "  return keras.preprocessing.sequence.pad_sequences(encoded, maxlen=max_length, padding='pad')\n",
    "\n",
    "# encode data\n",
    "X_train = encode(tokenizer, X_train_trimmed, length)\n",
    "X_test = encode(tokenizer, X_test_trimmed, length)\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_test shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbxZDwijZ4nE"
   },
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 8718, 100)         4927200   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 8711, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 4355, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 139360)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                1393610   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 6,346,453\n",
      "Trainable params: 6,346,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    Embedding(vocab_size, 100, input_length=max_length),\n",
    "    Conv1D(filters=32, kernel_size=8, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "126/126 - 119s - loss: 0.6983 - accuracy: 0.4888\n",
      "Epoch 2/10\n",
      "126/126 - 129s - loss: 0.6931 - accuracy: 0.5084\n",
      "Epoch 3/10\n",
      "126/126 - 141s - loss: 0.6577 - accuracy: 0.6209\n",
      "Epoch 4/10\n",
      "126/126 - 137s - loss: 0.3132 - accuracy: 0.8875\n",
      "Epoch 5/10\n",
      "126/126 - 127s - loss: 0.1419 - accuracy: 0.9580\n",
      "Epoch 6/10\n",
      "126/126 - 128s - loss: 0.1043 - accuracy: 0.9625\n",
      "Epoch 7/10\n",
      "126/126 - 127s - loss: 0.0820 - accuracy: 0.9630\n",
      "Epoch 8/10\n",
      "126/126 - 127s - loss: 0.0664 - accuracy: 0.9685\n",
      "Epoch 9/10\n",
      "126/126 - 121s - loss: 0.0628 - accuracy: 0.9662\n",
      "Epoch 10/10\n",
      "126/126 - 119s - loss: 0.0615 - accuracy: 0.9675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d23ba3d040>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X_train, y_train, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7QNm3bVZ8HB"
   },
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 50.081038\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPq3vB88YxHaEGJe+DQxcIG",
   "collapsed_sections": [
    "FnTLm04zkzFM"
   ],
   "mount_file_id": "1_75GavANC7xZ_tZNWujClqpI2YcYzewU",
   "name": "fakenews.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/ysthakur/fakenews/blob/main/fakenews.ipynb",
     "timestamp": 1625673719939
    }
   ],
   "version": ""
  },
  "interpreter": {
   "hash": "6ae64ceff864aff06dfe2387e03ab20c2f60d97330c4c57fa89b4903a526ea01"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
