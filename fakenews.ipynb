{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnTLm04zkzFM"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 161,
     "status": "ok",
     "timestamp": 1625785213216,
     "user": {
      "displayName": "Yash Thakur",
      "photoUrl": "",
      "userId": "12694896413693995597"
     },
     "user_tz": 240
    },
    "id": "e4KRoFt9rzQR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1625785214599,
     "user": {
      "displayName": "Yash Thakur",
      "photoUrl": "",
      "userId": "12694896413693995597"
     },
     "user_tz": 240
    },
    "id": "dSwxCMZunCO4",
    "outputId": "701eef7e-9e04-41ec-80f3-343cf71664ab"
   },
   "outputs": [],
   "source": [
    "# for pkg in ['stopwords', 'punkt', 'wordnet']:\n",
    "#   nltk.download(pkg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJ9B-DVQksPq"
   },
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1625787190589,
     "user": {
      "displayName": "Yash Thakur",
      "photoUrl": "",
      "userId": "12694896413693995597"
     },
     "user_tz": 240
    },
    "id": "A5JjA4pocyDC"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Split rows of train.csv into [text, label] if possible\n",
    "def split_text_label(text):\n",
    "  # The first group is the text, the second is the label at the end\n",
    "  match = re.match(r'^(.*)\\s+?([^\\s]+)$', text)\n",
    "  \n",
    "  if not match: raise ValueError(f'format wrong arg={text}')\n",
    "  \n",
    "  text = match.group(1)\n",
    "  label = match.group(2)\n",
    "  \n",
    "  if label == '1':\n",
    "    label = 1.0\n",
    "  elif label == '0':\n",
    "    label = 0.0\n",
    "  elif label == 'label':\n",
    "    return None # There's a line \"content label\" in the data\n",
    "  else:\n",
    "    raise ValueError(f'label wrong label={label}, text={text}')\n",
    "  \n",
    "  return [label, text.strip()]\n",
    "\n",
    "def mapl(f, list):\n",
    "  '''Map a list eagerly'''\n",
    "  return [f(elem) for elem in list]\n",
    "\n",
    "def load_csv(file_name):\n",
    "  # Data obtained from https://www.kaggle.com/c/fakenewskdd2020/data\n",
    "  with open(f'fakenewskdd2020/{file_name}.csv', 'r', encoding='utf8') as csv:\n",
    "    next(csv) # Skip the header with the column titles\n",
    "    return list(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8um1nfIZTEWt"
   },
   "outputs": [],
   "source": [
    "train_orig = mapl(split_text_label, load_csv('train'))\n",
    "train_orig = [res for res in train_orig if res]\n",
    "\n",
    "X_train_orig = mapl(lambda text_label: text_label[1], train_orig)\n",
    "y_train_orig = mapl(lambda text_label: text_label[0], train_orig)\n",
    "X_test_orig = mapl(lambda line: re.sub(re.compile('^\\\\d+\\t'), '', line), load_csv('test'))\n",
    "y_test_orig = mapl(lambda line: float(re.sub(re.compile('^\\\\d+,|\\n'), '', line)), load_csv('sample_submission'))\n",
    "\n",
    "assert len(X_train_orig) == len(y_train_orig)\n",
    "assert len(X_test_orig) == len(y_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there are an equal number of fake and true articles\n",
    "def balance_pos_neg(X, y):\n",
    "  num_vals = min(y.count(0.0), y.count(1.0))\n",
    "  pos = [0] * num_vals\n",
    "  neg = [0] * num_vals\n",
    "  pos_ind, neg_ind = 0, 0\n",
    "  for i in range(len(X)):\n",
    "    if y[i]:\n",
    "      if pos_ind < num_vals:\n",
    "        pos[pos_ind] = X[i]\n",
    "        pos_ind += 1\n",
    "    else:\n",
    "      if neg_ind < num_vals:\n",
    "        neg[neg_ind] = X[i]\n",
    "        neg_ind += 1\n",
    "  \n",
    "  # Associate each set of texts with their labels\n",
    "  pos = mapl(lambda text: [1.0, text], pos)\n",
    "  neg = mapl(lambda text: [0.0, text], neg)\n",
    "  \n",
    "  # Put them together into a DataFrame\n",
    "  df = pd.DataFrame(pos + neg, columns=['label', 'text'])\n",
    "  # Shuffle it just in case (from https://stackoverflow.com/a/34879805)\n",
    "  df = df.sample(frac=1).reset_index(drop=True)\n",
    "  \n",
    "  X = df['text'].to_list()\n",
    "  y = np.array(mapl(bool, df['label'].to_list()))\n",
    "\n",
    "  return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trimmed, y_train = balance_pos_neg(X_train_orig, y_train_orig)\n",
    "X_test_trimmed, y_test = balance_pos_neg(X_test_orig, y_test_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0ZAqfJHk4we"
   },
   "source": [
    "# Processing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Angad's LDA_Demo.ipynb\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "stemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(document):\n",
    "  # Remove all the special characters\n",
    "  document = re.sub(r'\\W', ' ', str(document))\n",
    "  # remove all single characters\n",
    "  document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "  # Remove single characters from the start\n",
    "  document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "  # Substituting multiple spaces with single space\n",
    "  document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "  # Converting to Lowercase\n",
    "  document = document.lower()\n",
    "  # Lemmatization\n",
    "  tokens = mapl(stemmer.lemmatize, document.split())\n",
    "  tokens = [word for word in tokens if len(word) > 3  and word not in en_stop]\n",
    "\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean = mapl(preprocess_text, X_train_trimmed)\n",
    "X_test_clean = mapl(preprocess_text, X_test_trimmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 7812\n",
      "Vocabulary size: 49272\n",
      "X_train shape: (4028, 7812), X_test shape: (1234, 7812)\n"
     ]
    }
   ],
   "source": [
    "# Much of the following is from https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_clean)\n",
    "\n",
    "max_len = max(max(map(len, X_test_clean)), max(map(len, X_test_clean)))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length:', max_len)\n",
    "print('Vocabulary size:', vocab_size)\n",
    "\n",
    "encode = lambda texts: keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(texts),\n",
    "    maxlen=max_len,\n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "X_train = encode(X_train_clean)\n",
    "X_test = encode(X_test_clean)\n",
    "print(f'X_train shape: {X_train.shape}, X_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbxZDwijZ4nE"
   },
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 7812, 100)         4927200   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 7805, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 3902, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 124864)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1248650   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 6,201,493\n",
      "Trainable params: 6,201,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    Embedding(vocab_size, 100, input_length=max_len),\n",
    "    Conv1D(filters=32, kernel_size=8, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "126/126 - 115s - loss: 0.7062 - accuracy: 0.4935\n",
      "Epoch 2/10\n",
      "126/126 - 108s - loss: 0.6796 - accuracy: 0.5598\n",
      "Epoch 3/10\n",
      "126/126 - 111s - loss: 0.5454 - accuracy: 0.7783\n",
      "Epoch 4/10\n",
      "126/126 - 117s - loss: 0.2219 - accuracy: 0.9536\n",
      "Epoch 5/10\n",
      "126/126 - 122s - loss: 0.1351 - accuracy: 0.9640\n",
      "Epoch 6/10\n",
      "126/126 - 122s - loss: 0.0978 - accuracy: 0.9677\n",
      "Epoch 7/10\n",
      "126/126 - 118s - loss: 0.0803 - accuracy: 0.9677\n",
      "Epoch 8/10\n",
      "126/126 - 119s - loss: 0.0678 - accuracy: 0.9685\n",
      "Epoch 9/10\n",
      "126/126 - 123s - loss: 0.0590 - accuracy: 0.9697\n",
      "Epoch 10/10\n",
      "126/126 - 120s - loss: 0.0553 - accuracy: 0.9717\n",
      "INFO:tensorflow:Assets written to: model_7-10-11-33\\assets\n"
     ]
    }
   ],
   "source": [
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X_train, y_train, epochs=10, verbose=2)\n",
    "model.save('model_7-10-11-33changethis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7QNm3bVZ8HB"
   },
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 49.432740\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPq3vB88YxHaEGJe+DQxcIG",
   "collapsed_sections": [
    "FnTLm04zkzFM"
   ],
   "mount_file_id": "1_75GavANC7xZ_tZNWujClqpI2YcYzewU",
   "name": "fakenews.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/ysthakur/fakenews/blob/main/fakenews.ipynb",
     "timestamp": 1625673719939
    }
   ],
   "version": ""
  },
  "interpreter": {
   "hash": "6ae64ceff864aff06dfe2387e03ab20c2f60d97330c4c57fa89b4903a526ea01"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
