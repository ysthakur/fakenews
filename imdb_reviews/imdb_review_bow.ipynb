{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b6e65bb-7a3a-4215-9a6d-6d05d2f750d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yasht\\fakenews\\venv\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "\n",
    "import random\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecfe15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e54af23-12df-47e8-b5e4-ccf1a3b12a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model_file = 'imdb_review_w2v.model' # change each time\n",
    "train_csv = 'train_df.csv'\n",
    "test_csv = 'test_df.csv'\n",
    "\n",
    "neg_bound = 4\n",
    "pos_bound = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba3b705-8678-47db-b1f4-422cadff51cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    \"\"\"Get the vector for a word\"\"\"\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except:\n",
    "        print(word)\n",
    "        raise\n",
    "        \n",
    "def filter_tokens(tokens):\n",
    "    return [token for token in tokens if token in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a40aa70d-76bf-4788-9356-c8f2a1df7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    # Remove non-word characters\n",
    "    text = re.sub(r'[^a-z]', ' ', text)\n",
    "    # Remove single letters\n",
    "    text = re.sub(r'\\b[a-z]{0,3}\\b', ' ', text)\n",
    "    # Merge multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Lemmatization\n",
    "    tokens = text.split()\n",
    "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in en_stop]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd27bbb",
   "metadata": {},
   "source": [
    "# Run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c7595b71-d97b-4a82-854f-ceae46ad7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_or_test(dir):\n",
    "    \"\"\"\n",
    "    Return the negative and positive train or test data\n",
    "    \"\"\"\n",
    "    def load_neg_or_pos(sub):\n",
    "        res = []\n",
    "        for file_name in os.listdir(sub):\n",
    "            with open(sub + file_name, encoding='utf8') as file:\n",
    "                underscore_ind = file_name.index('_')\n",
    "                period_ind = file_name.index('.')\n",
    "                id = int(file_name[:underscore_ind])\n",
    "                rating = int(file_name[underscore_ind + 1:period_ind])\n",
    "                text = next(file)\n",
    "                res.append([id, rating, text])\n",
    "        return res\n",
    "    # Only choose more polar ratings\n",
    "    neg = [[id, rating, text] for id, rating, text in load_neg_or_pos(dir + '/neg/') if rating <= neg_bound]\n",
    "    pos = [[id, rating, text] for id, rating, text in load_neg_or_pos(dir + '/pos/') if rating >= pos_bound]\n",
    "    random.shuffle(neg)\n",
    "    random.shuffle(pos)\n",
    "    both = neg[:4000] + pos[:4000]\n",
    "    random.shuffle(both)\n",
    "    return pd.DataFrame(both, columns=['Id', 'Rating', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ab31fc82-cf15-49a8-a3a2-ddcf787f3cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_train_or_test('./train')\n",
    "test_df = load_train_or_test('./test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9a53694d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5597</td>\n",
       "      <td>3</td>\n",
       "      <td>I will say that at least the movie makes sense...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "      <td>Zu Warriors most definitely should've been an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6374</td>\n",
       "      <td>10</td>\n",
       "      <td>Atlantis was much better than I had anticipate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4675</td>\n",
       "      <td>4</td>\n",
       "      <td>\"Washington Square\" is a flat, shabby adaptati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11382</td>\n",
       "      <td>3</td>\n",
       "      <td>Anyone notice that Tommy only has 3 facial exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>11970</td>\n",
       "      <td>4</td>\n",
       "      <td>...for the Lt to have chosen this one. First, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>4646</td>\n",
       "      <td>7</td>\n",
       "      <td>I didn't know what to make of this film. I gue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>10433</td>\n",
       "      <td>4</td>\n",
       "      <td>The fight scenes were great. Loved the old and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>10812</td>\n",
       "      <td>8</td>\n",
       "      <td>Japanese Tomo Akiyama's Keko Mask (1993) is ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>11951</td>\n",
       "      <td>8</td>\n",
       "      <td>Something happens to Sondra Pransky when she e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id  Rating                                               Text\n",
       "0      5597       3  I will say that at least the movie makes sense...\n",
       "1       156       1  Zu Warriors most definitely should've been an ...\n",
       "2      6374      10  Atlantis was much better than I had anticipate...\n",
       "3      4675       4  \"Washington Square\" is a flat, shabby adaptati...\n",
       "4     11382       3  Anyone notice that Tommy only has 3 facial exp...\n",
       "...     ...     ...                                                ...\n",
       "7995  11970       4  ...for the Lt to have chosen this one. First, ...\n",
       "7996   4646       7  I didn't know what to make of this film. I gue...\n",
       "7997  10433       4  The fight scenes were great. Loved the old and...\n",
       "7998  10812       8  Japanese Tomo Akiyama's Keko Mask (1993) is ex...\n",
       "7999  11951       8  Something happens to Sondra Pransky when she e...\n",
       "\n",
       "[8000 rows x 3 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0a80634a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 4000)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df[train_df['Rating'] <= neg_bound]), len(train_df[train_df['Rating'] >= pos_bound])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f849f8d-e4e8-4e90-b67a-a032dcff6533",
   "metadata": {},
   "source": [
    "## Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fe27c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Tokens'] = train_df['Text'].apply(tokenize)\n",
    "\n",
    "train_df['Text'] = train_df['Tokens'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7fa98e33-e3b9-4b96-b405-83a6d7dcbd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train and save model\n",
    "# model = Word2Vec(sentences=train_df['Tokens'])\n",
    "# model.save(w2v_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4d445870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = set(model.wv.key_to_index.keys())\n",
    "# vocab_ord = np.array(list(model.wv.key_to_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ab274690-2bae-4dba-8339-7e700c626e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keep only tokens that showed up the required number of times\n",
    "# train_df['Tokens'] = train_df['Tokens'].apply(filter_tokens)\n",
    "\n",
    "# test_df['Tokens'] = test_df['Text'].apply(lambda text: filter_tokens(tokenize(text)))\n",
    "# # Process test text too\n",
    "# test_df['Text'] = test_df['Tokens'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7e7bff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectors corresponding to each reviews' words\n",
    "train_df['Vectors'] = train_df['Tokens'].apply(get_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a95475f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "train_df.to_csv(train_csv)\n",
    "test_df.to_csv(test_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c63f1",
   "metadata": {},
   "source": [
    "# Load stuff done already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ae87b0b-22bc-48c6-a1be-82d12345457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Word2Vec.load(w2v_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02492993",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_csv)\n",
    "test_df = pd.read_csv(test_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b70547-0a53-4091-8a2e-9adb64d85a42",
   "metadata": {},
   "source": [
    "# Common stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "408dfb54-25c0-4ddf-a1bd-0979f7a3cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = set(model.wv.key_to_index.keys())\n",
    "# vocab_ord = np.array(list(model.wv.key_to_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec2d2b83-cb8c-42bf-9877-4dc5e5e6a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bi = train_df['Rating'] > 5\n",
    "y_test_bi = test_df['Rating'] > 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb9072",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d712b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vectorizer = CountVectorizer(min_df=5, stop_words=en_stop) # en_stop because the default has problems\n",
    "X_train_bow = cnt_vectorizer.fit_transform(train_df['Text'])\n",
    "X_test_bow = cnt_vectorizer.transform(test_df['Text'])\n",
    "\n",
    "# Scale data\n",
    "scaler_bow = StandardScaler(with_mean=False).fit(X_train_bow)\n",
    "X_train_bow_scaled = scaler_bow.transform(X_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80f8aee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 12665), 12665)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow.shape, len(cnt_vectorizer.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3d84446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_bow = LogisticRegression()\n",
    "lr_bow.fit(X_train_bow_scaled, y_train_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "79eb0a92-cb1f-4934-9fa3-630f8b4bf103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 2563, FN: 235\n",
      "FP: 1437, TN: 3765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.64      0.92      0.75      2798\n",
      "        True       0.94      0.72      0.82      5202\n",
      "\n",
      "    accuracy                           0.79      8000\n",
      "   macro avg       0.79      0.82      0.79      8000\n",
      "weighted avg       0.84      0.79      0.80      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bow_predicted = lr_bow.predict(X_test_bow)\n",
    "cm = confusion_matrix(bow_predicted, y_test_bi)\n",
    "print(f\"TP: {cm[0][0]}, FN: {cm[0][1]}\\nFP: {cm[1][0]}, TN: {cm[1][1]}\")\n",
    "print(classification_report(bow_predicted, y_test_bi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb06521-27c6-4752-b371-2b83b1ea49fe",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fa7880e-c99a-4085-9426-051eb3f642ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=5, stop_words=en_stop) # en_stop because the default has problems\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['Text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "270d982a-a799-43f1-84a6-4dd43c3c3cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 12665), 12665)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape, len(tfidf_vectorizer.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2064790-b6c7-4c54-94de-ae55a3fc7f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tfidf = LogisticRegression()\n",
    "lr_tfidf.fit(X_train_tfidf, y_train_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1562efa9-216e-4d8e-8b3b-fc367c6b6cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 3437, FN: 562\n",
      "FP: 563, TN: 3438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.86      0.86      3999\n",
      "        True       0.86      0.86      0.86      4001\n",
      "\n",
      "    accuracy                           0.86      8000\n",
      "   macro avg       0.86      0.86      0.86      8000\n",
      "weighted avg       0.86      0.86      0.86      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_predicted = lr_tfidf.predict(X_test_tfidf)\n",
    "cm = confusion_matrix(tfidf_predicted, y_test_bi)\n",
    "print(f\"TP: {cm[0][0]}, FN: {cm[0][1]}\\nFP: {cm[1][0]}, TN: {cm[1][1]}\")\n",
    "print(classification_report(tfidf_predicted, y_test_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907196fd-354f-491e-b45a-78fee71d7d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
