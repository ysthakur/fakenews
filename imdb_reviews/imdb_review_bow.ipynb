{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b6e65bb-7a3a-4215-9a6d-6d05d2f750d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yasht\\fakenews\\venv\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "\n",
    "import random\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecfe15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e54af23-12df-47e8-b5e4-ccf1a3b12a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model_file = 'imdb_review_w2v.model' # change each time\n",
    "train_csv = 'train_df.csv'\n",
    "test_csv = 'test_df.csv'\n",
    "df_csv = 'df.csv'\n",
    "\n",
    "neg_bound = 4\n",
    "pos_bound = 7\n",
    "\n",
    "train_size = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba3b705-8678-47db-b1f4-422cadff51cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    \"\"\"Get the vector for a word\"\"\"\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except:\n",
    "        print(word)\n",
    "        raise\n",
    "        \n",
    "def filter_tokens(tokens):\n",
    "    return [token for token in tokens if token in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40aa70d-76bf-4788-9356-c8f2a1df7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    # Remove non-word characters\n",
    "    text = re.sub(r'[^a-z]', ' ', text)\n",
    "    # Remove single letters\n",
    "    text = re.sub(r'\\b[a-z]{0,3}\\b', ' ', text)\n",
    "    # Merge multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Lemmatization\n",
    "    tokens = text.split()\n",
    "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in en_stop]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd27bbb",
   "metadata": {},
   "source": [
    "# Run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c7595b71-d97b-4a82-854f-ceae46ad7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_or_test(dir):\n",
    "    \"\"\"\n",
    "    Return the negative and positive train or test data\n",
    "    \"\"\"\n",
    "    def load_neg_or_pos(sub):\n",
    "        res = []\n",
    "        for file_name in os.listdir(sub):\n",
    "            with open(sub + file_name, encoding='utf8') as file:\n",
    "                underscore_ind = file_name.index('_')\n",
    "                period_ind = file_name.index('.')\n",
    "                id = int(file_name[:underscore_ind])\n",
    "                rating = int(file_name[underscore_ind + 1:period_ind])\n",
    "                text = next(file)\n",
    "                res.append([rating, text])\n",
    "        return res\n",
    "    # Only choose more polar ratings\n",
    "    neg = [[rating, text] for rating, text in load_neg_or_pos(dir + '/neg/') if rating <= neg_bound]\n",
    "    pos = [[rating, text] for rating, text in load_neg_or_pos(dir + '/pos/') if rating >= pos_bound]\n",
    "    random.shuffle(neg)\n",
    "    random.shuffle(pos)\n",
    "    both = neg[:8000] + pos[:8000]\n",
    "    random.shuffle(both)\n",
    "    return pd.DataFrame(both, columns=['Rating', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab31fc82-cf15-49a8-a3a2-ddcf787f3cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_train_or_test('./train') #.append(load_train_or_test('./test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a53694d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Now i have never ever seen a bad movie in all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Clint Eastwood returns as Dirty Harry Calahan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>RKO studios decided to borrow both William Pow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The subject notwithstanding, this is an amateu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>I watched this movie purely for the setting. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>9</td>\n",
       "      <td>A riotous farce set in the world of glamorous ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>8</td>\n",
       "      <td>Having first achieved fame with Drunken Master...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>1</td>\n",
       "      <td>...but a lousy film. As Maltin says this was C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>2</td>\n",
       "      <td>just watched it, me and my better half could n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>7</td>\n",
       "      <td>*May contain spoilers* *May contain spoilers*&lt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Rating                                               Text\n",
       "0           1  Now i have never ever seen a bad movie in all ...\n",
       "1          10  Clint Eastwood returns as Dirty Harry Calahan ...\n",
       "2           9  RKO studios decided to borrow both William Pow...\n",
       "3           1  The subject notwithstanding, this is an amateu...\n",
       "4           2  I watched this movie purely for the setting. I...\n",
       "...       ...                                                ...\n",
       "15995       9  A riotous farce set in the world of glamorous ...\n",
       "15996       8  Having first achieved fame with Drunken Master...\n",
       "15997       1  ...but a lousy film. As Maltin says this was C...\n",
       "15998       2  just watched it, me and my better half could n...\n",
       "15999       7  *May contain spoilers* *May contain spoilers*<...\n",
       "\n",
       "[16000 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a80634a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 8000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['Rating'] <= neg_bound]), len(df[df['Rating'] >= pos_bound])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f849f8d-e4e8-4e90-b67a-a032dcff6533",
   "metadata": {},
   "source": [
    "## Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe27c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tokens'] = df['Text'].apply(tokenize)\n",
    "# Clean up the text too\n",
    "df['Text'] = df['Tokens'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7fa98e33-e3b9-4b96-b405-83a6d7dcbd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train and save model\n",
    "# model = Word2Vec(sentences=train_df['Tokens'])\n",
    "# model.save(w2v_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d445870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = set(model.wv.key_to_index.keys())\n",
    "# vocab_ord = np.array(list(model.wv.key_to_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab274690-2bae-4dba-8339-7e700c626e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keep only tokens that showed up the required number of times\n",
    "# train_df['Tokens'] = train_df['Tokens'].apply(filter_tokens)\n",
    "\n",
    "# test_df['Tokens'] = test_df['Text'].apply(lambda text: filter_tokens(tokenize(text)))\n",
    "# # Process test text too\n",
    "# test_df['Text'] = test_df['Tokens'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7e7bff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The vectors corresponding to each reviews' words\n",
    "# df['Vectors'] = df['Tokens'].apply(get_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a95475f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "df.to_csv(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c63f1",
   "metadata": {},
   "source": [
    "# Load stuff done already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ae87b0b-22bc-48c6-a1be-82d12345457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Word2Vec.load(w2v_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02492993",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b70547-0a53-4091-8a2e-9adb64d85a42",
   "metadata": {},
   "source": [
    "# Common stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "408dfb54-25c0-4ddf-a1bd-0979f7a3cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = set(model.wv.key_to_index.keys())\n",
    "# vocab_ord = np.array(list(model.wv.key_to_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ec2d2b83-cb8c-42bf-9877-4dc5e5e6a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bi = df['Rating'] > 5\n",
    "y_train_bi, y_test_bi = train_test_split(y_bi, train_size=train_size, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb9072",
   "metadata": {},
   "source": [
    "# Logistic Regression + Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3f4a46df-3bb7-4c34-af14-4479d1eb575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_bow(**kwargs):\n",
    "    cnt_vectorizer = CountVectorizer(stop_words=en_stop, **kwargs) # en_stop because the default has problems\n",
    "    X_bow = cnt_vectorizer.fit_transform(df['Text'])\n",
    "    X_train_bow, X_test_bow = train_test_split(X_bow, train_size=train_size, random_state=1)\n",
    "\n",
    "    # Scale data\n",
    "    scaler_bow = StandardScaler(with_mean=False).fit(X_train_bow)\n",
    "    X_train_bow_scaled = scaler_bow.transform(X_train_bow)\n",
    "    X_test_bow_scaled = scaler_bow.transform(X_test_bow)\n",
    "    print(X_train_bow_scaled.shape, len(cnt_vectorizer.vocabulary_.keys()))\n",
    "    \n",
    "    lr_bow = LogisticRegression()\n",
    "    lr_bow.fit(X_train_bow_scaled, y_train_bi)\n",
    "\n",
    "    bow_predicted = lr_bow.predict(X_test_bow_scaled)\n",
    "    cm = confusion_matrix(bow_predicted, y_test_bi)\n",
    "    print(f\"TP: {cm[0][0]}, FN: {cm[0][1]}\\nFP: {cm[1][0]}, TN: {cm[1][1]}\")\n",
    "    print(classification_report(bow_predicted, y_test_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2408e0fc-febf-493c-b9c1-34071d65127f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12800, 18626) 18626\n",
      "TP: 1284, FN: 264\n",
      "FP: 269, TN: 1383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      0.83      0.83      1548\n",
      "        True       0.84      0.84      0.84      1652\n",
      "\n",
      "    accuracy                           0.83      3200\n",
      "   macro avg       0.83      0.83      0.83      3200\n",
      "weighted avg       0.83      0.83      0.83      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_bow(min_df=5, ngram_range=(1, 1)) # Just unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3bd5af37-5983-433a-b714-b00b1dbd8fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12800, 34146) 34146\n",
      "TP: 1176, FN: 318\n",
      "FP: 377, TN: 1329\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      0.79      0.77      1494\n",
      "        True       0.81      0.78      0.79      1706\n",
      "\n",
      "    accuracy                           0.78      3200\n",
      "   macro avg       0.78      0.78      0.78      3200\n",
      "weighted avg       0.78      0.78      0.78      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_bow(min_df=5, ngram_range=(2, 2)) # Just bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1072c298-f3ce-4087-a40e-493530dc786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12800, 52772) 52772\n",
      "TP: 1308, FN: 219\n",
      "FP: 245, TN: 1428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.86      0.85      1527\n",
      "        True       0.87      0.85      0.86      1673\n",
      "\n",
      "    accuracy                           0.85      3200\n",
      "   macro avg       0.85      0.86      0.85      3200\n",
      "weighted avg       0.86      0.85      0.86      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_bow(min_df=5, ngram_range=(1, 2)) # Unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "27d89c95-8810-4ffb-98cd-b7d6b96d4341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12800, 55247) 55247\n",
      "TP: 1313, FN: 218\n",
      "FP: 240, TN: 1429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.86      0.85      1531\n",
      "        True       0.87      0.86      0.86      1669\n",
      "\n",
      "    accuracy                           0.86      3200\n",
      "   macro avg       0.86      0.86      0.86      3200\n",
      "weighted avg       0.86      0.86      0.86      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_bow(min_df=5, ngram_range=(1, 3)) # Unigrams, bigrams, and trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb06521-27c6-4752-b371-2b83b1ea49fe",
   "metadata": {},
   "source": [
    "# Logistic Regression + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d72a3ee9-3459-42f7-8e15-5f1c59e7ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_tfidf(**kwargs):\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=en_stop, **kwargs) # en_stop because the default has problems\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df['Text'])\n",
    "    X_train_tfidf, X_test_tfidf = train_test_split(X_tfidf, train_size=train_size, random_state=1)\n",
    "\n",
    "    print(X_train_tfidf.shape, len(tfidf_vectorizer.vocabulary_.keys()))\n",
    "    \n",
    "    lr_tfidf = LogisticRegression()\n",
    "    lr_tfidf.fit(X_train_tfidf, y_train_bi)\n",
    "    \n",
    "    # Test\n",
    "    tfidf_predicted = lr_tfidf.predict(X_test_tfidf)\n",
    "    cm = confusion_matrix(tfidf_predicted, y_test_bi)\n",
    "    print(f\"TP: {cm[0][0]}, FN: {cm[0][1]}\\nFP: {cm[1][0]}, TN: {cm[1][1]}\")\n",
    "    print(classification_report(tfidf_predicted, y_test_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9d87f23a-21ae-4805-ad27-e58cb459ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12800, 53018) 53018\n",
      "TP: 1365, FN: 194\n",
      "FP: 188, TN: 1453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.88      0.88      1559\n",
      "        True       0.88      0.89      0.88      1641\n",
      "\n",
      "    accuracy                           0.88      3200\n",
      "   macro avg       0.88      0.88      0.88      3200\n",
      "weighted avg       0.88      0.88      0.88      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_tfidf(ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fb1c63e4-7843-4a89-883d-9993d5703ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12800, 1144585) 1144585\n",
      "TP: 1341, FN: 195\n",
      "FP: 212, TN: 1452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.87      0.87      1536\n",
      "        True       0.88      0.87      0.88      1664\n",
      "\n",
      "    accuracy                           0.87      3200\n",
      "   macro avg       0.87      0.87      0.87      3200\n",
      "weighted avg       0.87      0.87      0.87      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_tfidf(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8411dfc5-bbf8-49c7-befb-6f619fff25cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12800, 1091567) 1091567\n",
      "TP: 1207, FN: 202\n",
      "FP: 346, TN: 1445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.86      0.81      1409\n",
      "        True       0.88      0.81      0.84      1791\n",
      "\n",
      "    accuracy                           0.83      3200\n",
      "   macro avg       0.83      0.83      0.83      3200\n",
      "weighted avg       0.83      0.83      0.83      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_tfidf(ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e1047c-5dc6-4d2f-9b80-b64d69945e76",
   "metadata": {},
   "source": [
    "# Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ea0d9-3fbd-4548-9be8-7936f17576bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
